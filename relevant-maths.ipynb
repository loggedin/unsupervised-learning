{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3575e1a3-3ea5-424f-8ed2-30767035732d",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a128024-2859-477e-8e2f-9056c9728ff4",
   "metadata": {},
   "source": [
    "**Loadings vectors for all principal components for data set $\\pmb{x}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249ac05-065f-45a2-8a83-bf18f448120c",
   "metadata": {},
   "source": [
    "From Section 1.2.1 of the lecture notes, the normalised eigenvectors of $\\Sigma_x$ and corresponding eigenvalues $\\lambda_1, \\lambda_2$ ($\\lambda_1 \\geq \\lambda_2$) are the loadings vectors $\\pmb{c}_1, \\pmb{c}_2$ and corresponding sample variances of the first and second principal components, respectively.\n",
    "\n",
    "The characteristic polynomial is\n",
    "\n",
    "$$\n",
    "| \\,\\Sigma_x - \\lambda I_2 \\, | = \\begin{vmatrix}\n",
    "\\frac{1}{4} - \\lambda & 0 \\\\\n",
    "0 & 1 - \\lambda\n",
    "\\end{vmatrix} = \\left(\\frac{1}{4} - \\lambda\\right) \\left(1 - \\lambda\\right).\n",
    "$$\n",
    "\n",
    "The roots of this quadratic give the set of eigenvalues $\\sigma_{\\Sigma_x}=\\{1, \\frac{1}{4}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185d2c8-357b-4f8c-86e8-81b086c881d3",
   "metadata": {},
   "source": [
    "Let $\\lambda_1 = 1$, then $(\\Sigma_x - I_2) \\, \\pmb{c}_1 = \\pmb{0}$ gives $\\pmb{c}_{1_{1,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_1 = 1$ and loadings vector of the first principal component is\n",
    "\n",
    "$$\n",
    "\\pmb{c}_1 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08067d-890b-45ce-8850-4d0396ad3280",
   "metadata": {},
   "source": [
    "Let $\\lambda_2 = \\frac{1}{4}$, then $(\\Sigma_x - \\frac{1}{4} I_2) \\, \\pmb{c}_2 = \\pmb{0}$ gives $\\pmb{c}_{2_{2,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_2 = \\frac{1}{4}$ and loadings vector of the second principal component is\n",
    "\n",
    "$$\n",
    "\\pmb{c}_2 = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a39ee7-a0c1-47e7-bb8b-451563084f33",
   "metadata": {},
   "source": [
    "**Loadings vectors for all principal components for data set $\\pmb{y}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c5b91-77ef-4c17-a4cd-2d1db17e310f",
   "metadata": {},
   "source": [
    "From Section 1.2.1 of the lecture notes, the normalised eigenvectors of $\\Sigma_y$ and corresponding eigenvalues $\\lambda_1, \\lambda_2$ ($\\lambda_1 \\geq \\lambda_2$) are the loadings vectors $\\pmb{c}_1, \\pmb{c}_2$ and corresponding sample variances of the first and second principal components, respectively.\n",
    "\n",
    "The characteristic polynomial is\n",
    "\n",
    "$$\n",
    "| \\,\\Sigma_y - \\lambda I_2 \\, | = \\begin{vmatrix}\n",
    "1 - \\lambda & 0 \\\\\n",
    "0 & \\frac{1}{2} - \\lambda\n",
    "\\end{vmatrix} = \\left(1 - \\lambda\\right) \\left(\\frac{1}{2} - \\lambda\\right).\n",
    "$$\n",
    "\n",
    "The roots of this quadratic give the set of eigenvalues $\\sigma_{\\Sigma_y}=\\{1, \\frac{1}{2}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aacc9-58f3-43a4-a880-d63f7e8ff2d1",
   "metadata": {},
   "source": [
    "Let $\\lambda_1 = 1$, then $(\\Sigma_y - I_2) \\, \\pmb{c}_1 = \\pmb{0}$ gives $\\pmb{c}_{1_{2,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_1 = 1$ and loadings vector of the first principal component is\n",
    "\n",
    "$$\n",
    "\\pmb{c}_1 = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18a043-32d9-4ed6-a44b-7b3cd0183b8a",
   "metadata": {},
   "source": [
    "Let $\\lambda_2 = \\frac{1}{2}$, then $(\\Sigma_y - \\frac{1}{2} I_2) \\, \\pmb{c}_2 = \\pmb{0}$ gives $\\pmb{c}_{2_{1,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_2 = \\frac{1}{2}$ and loadings vector of the second principal component is\n",
    "\n",
    "$$\n",
    "\\pmb{c}_2 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54ba8b-8b2e-4f90-bd6c-2661cd876698",
   "metadata": {},
   "source": [
    "**Canonical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a42ec2-ca48-436d-a290-ad83fba9a9e2",
   "metadata": {},
   "source": [
    "From Section 1.6.2 of the lecture notes, $\\pmb{a}_k$ is the normalised eigenvector of $\\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T$ corresponding to its $k$-th largest eigenvalue $\\lambda_k$ and $\\pmb{b}_k = \\Sigma_y^{-1} \\Sigma_{xy}^T \\pmb{a}_k$.\n",
    "\n",
    "$$\n",
    "\\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T = \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 & \\overline{\\gamma} \\\\\n",
    "\\gamma & 0\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 & \\gamma \\\\\n",
    "\\overline{\\gamma} & 0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "8 \\overline{\\gamma}^2 & 0 \\\\\n",
    "0 & \\gamma^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The characteristic polynomial is\n",
    "\n",
    "$$\n",
    "| \\, \\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T - \\lambda I_2 \\, | = \\begin{vmatrix}\n",
    "8 \\overline{\\gamma}^2 - \\lambda & 0 \\\\\n",
    "0 & \\gamma^2 - \\lambda\n",
    "\\end{vmatrix} = \\left(8 \\overline{\\gamma}^2 - \\lambda \\right) \\left( \\gamma^2 - \\lambda \\right).\n",
    "$$\n",
    "\n",
    "The roots of this quadratic give the set of eigenvalues $\\sigma_{\\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T}=\\{8 \\overline{\\gamma}^2, \\gamma^2\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec2cb3-e065-41ca-b4ab-8f5813a84d50",
   "metadata": {},
   "source": [
    "### i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ce7c7-795f-4d5e-a495-5d0120d26774",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sqrt{8} \\overline{\\gamma} > \\gamma \\implies 8 \\overline{\\gamma}^2 > \\gamma ^2\n",
    "$$\n",
    "\n",
    "Therefore, when $\\sqrt{8} \\overline{\\gamma} > \\gamma$, the largest eigenvalue is $\\lambda_1 = 8 \\overline{\\gamma}^2$ and the second largest eigenvalue is $\\lambda_2 = \\gamma ^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bbb38-e247-4d82-935f-ae04aa5b9067",
   "metadata": {},
   "source": [
    "$\\left( \\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T - 8 \\overline{\\gamma}^2 I_2 \\right) \\pmb{a}_1 = \\pmb{0}$ gives $\\pmb{a}_{1_{2,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_1 = 8 \\overline{\\gamma}^2$ is\n",
    "\n",
    "$$\n",
    "\\pmb{a}_1 = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6586e-3ba0-468f-96c5-16396a9bb27c",
   "metadata": {},
   "source": [
    "It then follows that\n",
    "\n",
    "$$\n",
    "\\pmb{b}_1 = \\Sigma_y^{-1} \\Sigma_{xy}^T \\pmb{a}_1 = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 & \\gamma \\\\\n",
    "\\overline{\\gamma} & 0\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "2 \\overline{\\gamma}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d43df-7953-4c6a-a801-75db507b610c",
   "metadata": {},
   "source": [
    "The first pair of canonical variables is $u_{1, i} = \\pmb{a}_1^T \\pmb{x}_i = \\pmb{x}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ and $v_{1, i} = \\pmb{b}_1^T \\pmb{y}_i = 2 \\overline{\\gamma} \\pmb{y}_{i_{2, 1}}, \\  i = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d919a-2009-4195-9756-cbcaa3814f35",
   "metadata": {},
   "source": [
    "$\\left( \\Sigma_x^{-1} \\Sigma_{xy} \\Sigma_y^{-1} \\Sigma_{xy}^T - \\gamma^2 I_2 \\right) \\pmb{a}_2 = \\pmb{0}$ gives $\\pmb{a}_{2_{1,1}}=0$. So the normalised eigenvector corresponding to $\\lambda_2 = \\gamma ^2$ is\n",
    "\n",
    "$$\n",
    "\\pmb{a}_2 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccc328-04ff-420d-b565-116ead3fb5f4",
   "metadata": {},
   "source": [
    "It then follows that\n",
    "\n",
    "$$\n",
    "\\pmb{b}_2 = \\Sigma_y^{-1} \\Sigma_{xy}^T \\pmb{a}_2 = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 & \\gamma \\\\\n",
    "\\overline{\\gamma} & 0\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\gamma \\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413fb0d-0ad3-4008-8b9b-96d0ab432622",
   "metadata": {},
   "source": [
    "The second pair of canonical variables is $u_{2, i} = \\pmb{a}_2^T \\pmb{x}_i = \\pmb{x}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ and $v_{2, i} = \\pmb{b}_2^T \\pmb{y}_i = \\gamma \\pmb{y}_{i_{1, 1}}, \\  i = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56922303-a81e-4664-af03-e202a10ab264",
   "metadata": {},
   "source": [
    "**Relationships between the principal components and the corresponding canonical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccb54d-4578-4132-9524-1b74ff845d7c",
   "metadata": {},
   "source": [
    "In the case where $\\sqrt{8} \\overline{\\gamma} > \\gamma$, the canonical variables show the following alignments with the principal components.\n",
    "\n",
    "For data set $\\pmb{x}$:\n",
    "\n",
    "The first canonical variable $u_{1, i} = \\pmb{x}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the second principal component.\n",
    "\n",
    "The second canonical variable $u_{2, i} = \\pmb{x}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the first principal component.\n",
    "\n",
    "For data set $\\pmb{y}$:\n",
    "\n",
    "The first canonical variable $v_{1, i} = 2 \\overline{\\gamma} \\pmb{y}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the second principal component (scaled by $2 \\overline{\\gamma}$).\n",
    "\n",
    "The second canonical variable $v_{2, i} = \\gamma \\pmb{y}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the first principal component (scaled by $\\gamma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017fcf5-f06f-4e24-86b5-f0da689077f0",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f5ea5-8b56-48ff-b382-66200f34d95f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sqrt{8} \\overline{\\gamma} < \\gamma \\implies 8 \\overline{\\gamma}^2 < \\gamma ^2 \n",
    "$$\n",
    "\n",
    "Therefore, when $\\sqrt{8} \\overline{\\gamma} < \\gamma$, the largest eigenvalue is $\\lambda_1 = \\gamma ^2$ and the second largest eigenvalue is $\\lambda_2 = 8 \\overline{\\gamma}^2$. It then follows that\n",
    "\n",
    "$$\n",
    "\\pmb{a}_1 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, \\pmb{b}_1 = \\begin{bmatrix}\n",
    "\\gamma \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\pmb{a}_2 = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}, \\pmb{b}_2 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "2 \\overline{\\gamma}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a2cc5-f63d-4ab1-889f-cc9eb8ce4350",
   "metadata": {},
   "source": [
    "The first pair of canonical variables is $u_{1, i} = \\pmb{a}_1^T \\pmb{x}_i = \\pmb{x}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ and $v_{1, i} = \\pmb{b}_1^T \\pmb{y}_i = \\gamma \\pmb{y}_{i_{1, 1}}, \\  i = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa91d19-3578-4337-b971-d4c864f00104",
   "metadata": {},
   "source": [
    "The second pair of canonical variables is $u_{2, i} = \\pmb{a}_2^T \\pmb{x}_i = \\pmb{x}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ and $v_{2, i} = \\pmb{b}_2^T \\pmb{y}_i = 2 \\overline{\\gamma} \\pmb{y}_{i_{2, 1}}, \\  i = 1, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915f86a-caf4-4226-a978-d98fbc5175c9",
   "metadata": {},
   "source": [
    "**Relationships between the principal components and the corresponding canonical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a5598-c0c3-4bd7-8c83-4cc1fc371665",
   "metadata": {},
   "source": [
    "In the case where $\\sqrt{8} \\overline{\\gamma} < \\gamma$, the canonical variables show the following alignments with the principal components.\n",
    "\n",
    "For data set $\\pmb{x}$:\n",
    "\n",
    "The first canonical variable $u_{1, i} = \\pmb{x}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the first principal component.\n",
    "\n",
    "The second canonical variable $u_{2, i} = \\pmb{x}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the second principal component.\n",
    "\n",
    "For data set $\\pmb{y}$:\n",
    "\n",
    "The first canonical variable $v_{1, i} = \\gamma \\pmb{y}_{i_{1, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the first principal component (scaled by $\\gamma$).\n",
    "\n",
    "The second canonical variable $v_{2, i} = 2 \\overline{\\gamma} \\pmb{y}_{i_{2, 1}}, \\  i = 1, \\ldots, n$ aligns with the projection in the direction of the second principal component (scaled by $2 \\overline{\\gamma}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aa9bd-4347-4ae7-bd63-f9665e89a6f8",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0007eb-5ae3-4437-bf1f-5bf453ca659a",
   "metadata": {},
   "source": [
    "### i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ff354-2cf4-4063-9edb-4b70f7dbabc8",
   "metadata": {},
   "source": [
    "$$\n",
    "h(f_{\\text{Unif}}) = -\\int_a^b f_{\\text{Unif}}(x) \\, \\text{log} \\, f_{\\text{Unif}}(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) = -\\int_a^b \\frac{1}{b-a} \\text{log} \\left( \\frac{1}{b-a} \\right) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) = - \\frac{1}{b-a} \\text{log} \\left( \\frac{1}{b-a} \\right) \\int_a^b dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) = - \\frac{1}{b-a} \\text{log} \\left( \\frac{1}{b-a} \\right) \\left( b-a \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) = - \\text{log} \\left( \\frac{1}{b-a} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) = \\text{log} \\, (b-a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddf1aa-84a0-4fcf-a184-d996a75f6444",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d4cdb-1d27-4a93-9d09-c8b2cf6475c4",
   "metadata": {},
   "source": [
    "Let $X$ denote a random variable with non-zero probability density function (pdf) $f$ on $[a, b]$.\n",
    "\n",
    "Define the random variable $Y = \\frac{g(X)}{f(X)}$, where $g$ is also a non-zero pdf on $[a, b]$.\n",
    "\n",
    "Since $x \\mapsto -\\log x$ is convex, we can use Jensen’s inequality.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[-\\text{log}(Y)] \\geq -\\text{log}(\\mathbb{E}[Y])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\mathbb{E}\\left[-\\text{log}\\left(\\frac{g(X)}{f(X)}\\right)\\right] \\geq -\\text{log}\\left(\\mathbb{E}\\left[\\frac{g(X)}{f(X)}\\right]\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b -\\text{log}\\left(\\frac{g(x)}{f(x)}\\right) f(x) \\, dx \\geq -\\text{log} \\left( \\int_a^b \\frac{g(x)}{f(x)} f(x) \\, dx \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b \\text{log}\\left(\\frac{f(x)}{g(x)}\\right) f(x) \\, dx \\geq -\\text{log} \\left( \\int_a^b g(x) \\, dx \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b f(x) \\, \\text{log}\\left(\\frac{f(x)}{g(x)}\\right) \\, dx \\geq -\\text{log} (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b f(x) \\, \\text{log}\\left(\\frac{f(x)}{g(x)}\\right) \\, dx \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77009e-05bf-4552-a995-9fd224023115",
   "metadata": {},
   "source": [
    "### iii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775995f1-1785-4096-8ec9-5a00de48c082",
   "metadata": {},
   "source": [
    "From (ii),\n",
    "\n",
    "$$\n",
    "\\int_a^b \\text{log}\\left(\\frac{f(x)}{f_{\\text{Unif}}(x)}\\right) f(x) \\, dx \\geq 0.\n",
    "$$\n",
    "\n",
    "Substitute $f_{\\text{Unif}}(x) = \\frac{1}{b-a}$ and simplify.\n",
    "\n",
    "$$\n",
    "\\int_a^b \\text{log}\\left(\\frac{f(x)}{\\left(\\frac{1}{b-a}\\right)}\\right) f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b \\text{log}\\Bigl( (b-a) f(x) \\Bigr) f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b \\Bigl( \\text{log}\\left( b-a \\right) + \\text{log} \\, f(x) \\Bigr) f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b \\Bigl( \\text{log}\\left( b-a \\right) f(x) + f(x) \\, \\text{log} \\, f(x) \\Bigr) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\int_a^b \\text{log}\\left( b-a \\right) f(x) \\, dx + \\int_a^b f(x) \\, \\text{log} \\, f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\text{log}\\left( b-a \\right) \\int_a^b f(x) \\, dx + \\int_a^b f(x) \\, \\text{log} \\, f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\text{log}\\left( b-a \\right) + \\int_a^b f(x) \\, \\text{log} \\, f(x) \\, dx \\geq 0\n",
    "$$\n",
    "\n",
    "From (i), $h(f_{\\text{Unif}}) = \\text{log} \\, (b-a)$. Also substitute $h(f) = -\\int_a^b f(x) \\, \\text{log} \\, f(x) \\, dx$.\n",
    "\n",
    "$$\n",
    "h(f_{\\text{Unif}}) - h(f) \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies h(f) \\leq h(f_{\\text{Unif}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28fd55-80c3-476d-98f0-aec36ea10ff6",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8115b1f-5610-4b5e-bbe9-2d076d7e592e",
   "metadata": {},
   "source": [
    "### i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2177c3f-2bd4-4c59-9777-323793682aa7",
   "metadata": {},
   "source": [
    "Study the behaviour of the function $f_x(y) = x \\ \\text{log} \\ y − y$ for fixed $x$.\n",
    "\n",
    "$$\n",
    "\\frac { \\partial f_x } { \\partial y } = \\frac {x} {y} - 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac { \\partial f_x } { \\partial y } = 0 \\implies y = x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac { \\partial^2 f_x } { \\partial y^2 } = - \\frac {x} {y^2}\n",
    "$$\n",
    "\n",
    "For $x > 0$, $\\frac { \\partial^2 f_x } { \\partial y^2 } < 0$ so $y=x$ maximises $f_x(y)$ for fixed $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c2f49-7c43-4d5e-a03a-97d9db5aed8b",
   "metadata": {},
   "source": [
    "$$\n",
    "L(W,H) = \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\Bigl( X_{i,j} \\log \\left( (W H)_{i,j} \\right) - (W H)_{i,j} \\Bigr)\n",
    "$$\n",
    "\n",
    "Since $X_{i,j} > 0$,\n",
    "\n",
    "$$\n",
    "L(W,H) \\leq \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\Bigl( X_{i,j} \\log X_{i,j} - X_{i,j} \\Bigr)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies L(W,H) \\leq \\sum_{i=1}^{d} \\sum_{j=1}^{n} X_{i,j} \\Bigl( \\log X_{i,j} - 1 \\Bigr).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda072a-d8eb-4b09-b788-a1cc6e02fad8",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76b2da-279e-4ab5-8094-27c0376cce1b",
   "metadata": {},
   "source": [
    "Apply a second-order Taylor expansion to the function $f_x(y) = x \\log y − y$ around $y=x$.\n",
    "\n",
    "$$\n",
    "f_x(y) \\approx f_x (x) + \\left. \\frac{ \\partial f_x }{ \\partial y } \\right|_{y=x} (y-x) + \\frac {1}{2} \\left. \\frac{ \\partial^2 f_x }{ \\partial y^2 } \\right|_{y=x} (y-x)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= x \\log x - x + \\left(\\frac{x}{x} - 1\\right) (y-x) + \\frac {1}{2} \\left(- \\frac {x} {x^2}\\right) (y-x)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= x \\log x - x - \\frac {1}{2x} (y-x)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209d59f-5568-42c5-b0ec-cc6cddfbbbb7",
   "metadata": {},
   "source": [
    "$$\n",
    "L(W^{(k)},H^{(k)}) = \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\left( X_{i,j} \\log \\left( (W^{(k)} H^{(k)})_{i,j} \\right) - (W^{(k)} H^{(k)})_{i,j} \\right)\n",
    "$$\n",
    "\n",
    "Apply a second-order Taylor expansion to $L(W^{(k)},H^{(k)})$ about $(W^{(k)} H^{(k)})_{i,j} = X_{i,j}$, noticing that the expression within the sums is the same form as $f_x(y)$.\n",
    "\n",
    "$$\n",
    "L(W^{(k)},H^{(k)}) \\approx \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\left( X_{i,j} \\log X_{i,j} - X_{i,j} - \\frac {1}{2 X_{i,j}} \\left( (W^{(k)} H^{(k)})_{i,j} - X_{i,j} \\right)^2 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\left( X_{i,j} \\log X_{i,j} - X_{i,j} \\right) - \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\frac {1}{2 X_{i,j}} \\left( (W^{(k)} H^{(k)})_{i,j} - X_{i,j} \\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{d} \\sum_{j=1}^{n} X_{i,j} \\left( \\log X_{i,j} - 1 \\right) - \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\frac {1}{2 X_{i,j}} \\left( X_{i,j} - (W^{(k)} H^{(k)})_{i,j} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2550b72-3ce6-4d13-bc75-a08b3859ff8e",
   "metadata": {},
   "source": [
    "We know that\n",
    "\n",
    "$$\n",
    "\\lim_{k \\to \\infty} D\\left( W^{(k)}, H^{(k)} \\right) = \\lim_{k \\to \\infty} \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\left( X_{i,j} - (W^{(k)} H^{(k)})_{i,j} \\right)^2 = 0.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "L(W^{(k)},H^{(k)}) = \\sum_{i=1}^{d} \\sum_{j=1}^{n} X_{i,j} \\left( \\text{log}X_{i,j} - 1 \\right) - \\mathcal{O}\\left( D\\left( W^{(k)}, H^{(k)} \\right) \\right),\n",
    "$$\n",
    "\n",
    "as $k \\to \\infty$, where\n",
    "\n",
    "$$\n",
    "\\mathcal{O}\\left( D\\left( W^{(k)}, H^{(k)} \\right) \\right) = \\sum_{i=1}^{d} \\sum_{j=1}^{n} \\frac {1}{2 X_{i,j}} \\left( X_{i,j} - (W^{(k)} H^{(k)})_{i,j} \\right)^2\n",
    "$$\n",
    "\n",
    "goes to zero at least as fast as $D\\left( W^{(k)}, H^{(k)} \\right)$ when $k \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939cdbe-3ed9-45ba-a568-2f7ccee89715",
   "metadata": {},
   "source": [
    "### iii."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac504af-8be7-4026-8129-584e4f4a6fb2",
   "metadata": {},
   "source": [
    "The optimisation of the objective function $L(W, H)$ is closely related to the optimisation of the objective function $D(W, H)$. Minimising $D(W,H)$ causes $L(W, H)$ to approach its upper bound. Therefore, both objective functions are aligned in the sense that improving the approximation in terms of the squared Euclidean distance also improves the approximation in terms of the quasi-likelihood function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
